# Creating One-Liners with an N-gram Language Model

This project uses a corpus of humorous one-liner jokes to train an n-gram language model using NLTK. The model can generate new joke-like lines and estimate the likelihood of word sequences.
An n-gram language model built using NLTK for generating new, joke-like text

## What It Does
- Loads online one-liner corpus
- Tokenizes and preprocesses text
- Trains a Maximum Likelihood Estimator (MLE) model with different `n` values
- Generates text from seed phrases
- Evaluates token probabilities

## Tools Used
- Python
- NLTK
- Requests

## Status
ðŸš§ Work in progress â€“ logic is in place, results being tuned.
